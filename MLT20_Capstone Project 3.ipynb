{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L3T20 - Capstone Project 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries & loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load dataset  and key from imdb set\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = imdb.load_data(num_words = 10000)\n",
    "word_dict = imdb.get_word_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict for review generation\n",
    "\n",
    "offset=3   # word index offset\n",
    "word_dict = {k:(v + offset) for k, v in word_dict.items()}\n",
    "word_dict[\"<PAD>\"] = 0\n",
    "word_dict[\"<START>\"] = 1\n",
    "word_dict[\"<UNK>\"] = 2\n",
    "word_dict[\"<UNUSED>\"] = 3\n",
    "id_dict = {value:key for key, value in word_dict.items()}\n",
    "\n",
    "# Create review print function\n",
    "\n",
    "def print_review(review_data):\n",
    "    print('\\n=================================================')\n",
    "    print(f'Sample = {i} | Length = {len(review_data)}')\n",
    "    print('=================================================')\n",
    "    print(' '.join(id_dict[id] for id in review_data))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================\n",
      "Sample = 5326 | Length = 116\n",
      "=================================================\n",
      "<START> i realize this review will get me <UNK> by the expert film critics <UNK> this site but i will defend this film br br the dentist is actually a really good film the acting isn't always top notch but the thrills are good and the story's good plus you see linda <UNK> <UNK> not that i'm an expert in this field but the direction seems good and the plot makes sense corbin makes a great creepy dentist it does to <UNK> what jaws does to sharks ish it obviously had a fairly limited budget but they did well with it what they could and developed the characters well those that count br br the end\n",
      "\n",
      "=================================================\n",
      "Sample = 100 | Length = 158\n",
      "=================================================\n",
      "<START> i am a great fan of david lynch and have everything that he's made on dvd except for hotel room the 2 hour twin peaks movie so when i found out about this i immediately grabbed it and and what is this it's a bunch of <UNK> drawn black and white cartoons that are loud and foul mouthed and unfunny maybe i don't know what's good but maybe this is just a bunch of crap that was <UNK> on the public under the name of david lynch to make a few bucks too let me make it clear that i didn't care about the foul language part but had to keep <UNK> the sound because my neighbors might have all in all this is a highly disappointing release and may well have just been left in the <UNK> box set as a curiosity i highly recommend you don't spend your money on this 2 out of 10\n"
     ]
    }
   ],
   "source": [
    " # Print random reviews using for loops\n",
    "review_num = 2\n",
    "for i in np.random.randint(0, len(train_x), review_num):\n",
    "    print_review(train_x[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Data before using it in the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to  ensure compatibility with the modelling structures is to first process the data before it is to be used in the neural network. The processing required is that the reviews need to be truncated to 500 words and any reviews shorter are to be padded. This is to ensure so that all of the data is of a similar size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "train_x_padding = pad_sequences(train_x, maxlen = 500, value = 0)\n",
    "test_x_padding  = pad_sequences(test_x, maxlen = 500, value = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "np.random.seed(1984)\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 20, input_length = 500))\n",
    "model.add(LSTM(32, return_sequences = True))\n",
    "model.add(LSTM(16))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer of the neutral network  is the Embedding layer. This layer enables the neural network to operate on text data that are converted to integers. LSTM layers  allow the recurrence required in the RNN. Dropout was used to assist in preventing the overfitting of the model.\n",
    "\n",
    "The input to the model would be a vector of 500 integers as per the  data processing step. The output would be a number between 0 and 1 where 0 is a negative sentiment and 1 is a postive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 20)           200000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 500, 32)           6784      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 209,937\n",
      "Trainable params: 209,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 493s 20ms/sample - loss: 0.5391 - acc: 0.7334 - val_loss: 0.6894 - val_acc: 0.5014\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 476s 19ms/sample - loss: 0.6281 - acc: 0.6540 - val_loss: 0.6524 - val_acc: 0.6138\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 474s 19ms/sample - loss: 0.5254 - acc: 0.7376 - val_loss: 0.4071 - val_acc: 0.8181\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 473s 19ms/sample - loss: 0.3996 - acc: 0.8344 - val_loss: 0.4032 - val_acc: 0.8300\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 473s 19ms/sample - loss: 0.3425 - acc: 0.8642 - val_loss: 0.3761 - val_acc: 0.8373\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 473s 19ms/sample - loss: 0.2880 - acc: 0.8937 - val_loss: 1.1351 - val_acc: 0.5436\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 473s 19ms/sample - loss: 0.3169 - acc: 0.8756 - val_loss: 0.3564 - val_acc: 0.8537\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 473s 19ms/sample - loss: 0.2657 - acc: 0.9006 - val_loss: 0.3539 - val_acc: 0.8481\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 479s 19ms/sample - loss: 0.2448 - acc: 0.9134 - val_loss: 0.3828 - val_acc: 0.8532\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 475s 19ms/sample - loss: 0.1950 - acc: 0.9333 - val_loss: 0.3772 - val_acc: 0.8601\n"
     ]
    }
   ],
   "source": [
    "# Compiles the model\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Provides a summary\n",
    "model.summary()\n",
    "np.random.seed(1984)\n",
    "# Model training\n",
    "epochist = model.fit(train_x_padding , train_y, epochs = 10\n",
    "                     , verbose = True\n",
    "                     , validation_data = (test_x_padding, test_y)\n",
    "                     , batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.3772 - acc: 0.8601\n",
      "Test results: \n",
      " ==> Loss: 0.377\n",
      " ==> Accuracy: 86.012%\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_x_padding, test_y)\n",
    "Loss = np.round(results[0], 3)\n",
    "Acc = np.round(100*results[1], 3)\n",
    "print(f'Test results: \\n ==> Loss: {Loss}\\n ==> Accuracy: {Acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function the provides the model output for given riew\n",
    "def m_predict(review):\n",
    "    # Split review into words\n",
    "    split = review.lower().split()\n",
    "    \n",
    "    # Convert words to integers using word_to_id dictionary\n",
    "    ids = []\n",
    "    for k in split:\n",
    "        if word_dict.get(k) == None or word_dict.get(k) > 10000:\n",
    "            ids.append(2)\n",
    "        else:\n",
    "            ids.append(word_dict.get(k))\n",
    "            \n",
    "    # Pad and truncate to input length\n",
    "    ids_p = pad_sequences(np.array(list([ids])), maxlen = 500, value = 0)\n",
    "    \n",
    "    # Use model\n",
    "    score = model.predict(ids_p)\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie is good <==> [[0.7944853]] <==> Good Sentiment\n",
      "This movie is not good <==> [[0.64577883]] <==> Good Sentiment\n",
      "She liked the movie  <==> [[0.90852463]] <==> Good Sentiment\n",
      "She did not like the movie <==> [[0.53184646]] <==> Bad Sentiment\n",
      "Beautiful and exciting movie <==> [[0.925625]] <==> Good Sentiment\n",
      "Wonderful and interesting <==> [[0.88919955]] <==> Good Sentiment\n",
      "Depressing and sad <==> [[0.19502357]] <==> Bad Sentiment\n",
      "Boring and long <==> [[0.08598903]] <==> Bad Sentiment\n",
      "This movie is great <==> [[0.91242594]] <==> Good Sentiment\n",
      "This movie is not great <==> [[0.839977]] <==> Good Sentiment\n"
     ]
    }
   ],
   "source": [
    "# Test model with self-defined examples\n",
    "\n",
    "sample = ['This movie is good'\n",
    "            , 'This movie is not good'\n",
    "            , 'She liked the movie '\n",
    "            , 'She did not like the movie'\n",
    "            , 'Beautiful and exciting movie'\n",
    "            , 'Wonderful and interesting'\n",
    "            , 'Depressing and sad'\n",
    "            , 'Boring and long'\n",
    "            , 'This movie is great'\n",
    "            , 'This movie is not great'\n",
    "    \n",
    "]\n",
    "bench = 0.55\n",
    "\n",
    "for s in sample:\n",
    "    score = m_predict(s)\n",
    "    \n",
    "    if score > bench:\n",
    "        sent = 'Good'\n",
    "    else:\n",
    "        sent ='Bad'\n",
    "        \n",
    "    print(f'{s} <==> {score} <==> {sent} Sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various sample reviews were passed through the model. The main issue with the prediction of the model is that 'is not good' and'is not great' were identified as good  when it is infact bad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
